{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Markdown styles](https://towardsdatascience.com/enrich-your-jupyter-notebook-with-these-tips-55c8ead25255)\n",
    "\n",
    "[Hydra overview](https://towardsdatascience.com/stop-hard-coding-in-a-data-science-project-use-config-files-instead-479ac8ffc76f)\n",
    "\n",
    "<u>credits</u>: [Will Koehrsen](https://medium.com/p/4d028e6f0526)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"><u>Book Recommendation System</u></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "<h2>Introduction</h2>\n",
    "\n",
    "The main purpose of this work is to learn <font color='orange'>**embeddings**</font>. Embeddings can be considered as a representation of a document in a space (called **embedding space**) where similar documents should be very close to each other. Therefore to guide our comprehension of what are embeddings, we will build a recommendation system based on books in wikipedia. The principle is as follow: \"<u>*books with Wikipedia pages that link to similar Wikipedia pages are similar to each other*</u>\". Ex: Book1 <-- link2; Book3 <-- link2 => Book1 and Book3 are similar. So, to exhibit this similarity, we will use neural network entity embeddings, mapping each book and each Wikipedia link (Wikilink) to a 50-number vector.\n",
    "\n",
    "Embeddings have demonstrated some advantages in representing categorical variables: \n",
    "1. They help in reducing the original space where documents are expressed. In our case for example, if we were using *one-hot-encoding* method to representing books, we would have 3.7e+06 length vector for each book. \n",
    "2. They preserve similarity in terms of semantic meaning of each document (the books in our case) that is not the case of *one-hot-encoding* where semantic meaning of document is not preserved. By training a neural network to learn entity embeddings, we not only get a reduced dimension representation of the books, we also get a representation that keeps similar books (in terms of what they talk about) closer to each other. Basic approach for a recommendation system is to find the closest books for any book in other to recommend to a user books that may have same content like those he/she already read. Thanks to [this notebook](1-Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb), we have access to every single book article on Wikipedia, which will let us create an effective recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Approach</h2>\n",
    "\n",
    "To create entity embeddings, we need to <u>build an **embedding neural network**</u> and train it <u>on a **supervised machine learning task**</u>** that will <u>result in **similar books** (and similar links) having closer representations</u> in embedding space. <font color='orange'>The parameters of the neural network - the weights - are the embeddings</font>, and so during training, these numbers are adjusted to minimize the loss on the prediction problem. In other words, the network tries to accurately complete the task by changing the representations of the books and the links.\n",
    "\n",
    "Once we have the embeddings for the books and the links, we can find the most similar book to a given book by computing the distance between the embedded vector for that book and all the other book embeddings. <u>We'll use the **cosine distance**</u> which measures the angle between two vectors as a measure of similarity (another valid option is the Euclidean distance). We can also do the same with the links, finding the most similar page to a given page. (I use links and wikilinks interchangeably in this notebook). The steps we will follow are:\n",
    "\n",
    "1. Load in data and clean\n",
    "2. Prepare data for supervised machine learning task\n",
    "3. Build the entity embedding neural network\n",
    "4. Train the neural network on prediction task\n",
    "5. Extract embeddings and find most similar books and wikilinks\n",
    "6. Visualize the embeddings using dimension reduction techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question*: But what consists the Supervised Machine Learning Task ? ðŸ¤”<br/>\n",
    "*Answer*: Find the model that correctly <u>Maps Books to Links</u> ðŸ˜€\n",
    "\n",
    "For our machine learning task, we'll set up the problem as <u>identifying</u> <font color='green'>whether</font> <u>or</u> <font color='red'>not</font> <u>a particular link was present in a book article</u>. The training examples will consist of (book, link) pairs, with some pairs true examples - actually in the data - and others negative examples - do not occur in the data. It will be the network's job to adjust the entity embeddings (the neuralnets weights) of the books and the links in order to accurately make the classification. \n",
    "\n",
    "|                      Books                          | Links |  Link present/absent ?  |\n",
    "|:---------------------------------------------------:|:---------:|:------:|\n",
    "| Book4|   wikilink1   |  <font color='green'>Present</font>  |\n",
    "| Book4|   wikilink7   |  <font color='red'>Absent<font>   |\n",
    "| Book4|   wikilink8   |  <font color='green'>Present</font>  |\n",
    "| Book2|   wikilink7   |  <font color='red'>Absent<font>   |\n",
    "| Book2|   wikilink1   |  <font color='green'>Present</font>  |\n",
    "| Book5|   wikilink3   |  <font color='red'>Absent<font>   |\n",
    "| Book5|   wikilink5   |  <font color='red'>Absent<font>   |\n",
    "| Book5|   wikilink1   |  <font color='green'>Present</font>  |\n",
    "| Book1|   wikilink5   |  <font color='green'>Present</font>  |\n",
    "\n",
    "<u>**Keep in mind**</u>: Although we are training our neuralnet for a supervised machine learning task, our end objective is not to make accurate predictions on new data, but learn the best entitiy embeddings, so we do not use a validation or testing set. We use the prediction problem as a means to an end rather than the final outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Neural Network Embeddings for better representing large sequence of texts</h2>\n",
    "\n",
    "\n",
    "Neural Network embeddings have proven to be very powerful concepts both for modeling language and for representing categorical variables.  For example, the [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec?hl=fr) word embeddings by Google, map a word to a vector based on training a neural network on millions of words. These embeddings can be used in any supervised model because they are just numerical representations of categorical variables. Much as we one-hot-encode categorical variables to use them in a random forest for a supervised task, we can also use entity embeddings to include categorical variables in a model. The embeddings are also useful because we can find entities that are close to one another in embedding space which might - as in a book recommendation system - allow us to find the most similar categories among tens of thousands of choices.\n",
    "\n",
    "We can also use the Entity Embeddings to visualize words or categorical variables, such as creating a map of all books on Wikipedia. The entity embeddings typically are still high-dimensional - we'll use 50 numbers for each entity - so we need to use a dimension reduction technique such as [TSNE](https://distill.pub/2016/misread-tsne/) or [UMAP](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html) to visualize the embeddings in lower dimensions. (These are both manifold embedding methods so in effect we will embed the embeddings for visualization!) We'll take a look at doing this at the end of the notebook and later will upload the embeddings into a application custom-built for this purpose ([projector.tensorflow.org](https://projector.tensorflow.org/)). Entity embeddings are becoming more widespread thanks to the ease of development of neural networks in Keras and are a useful approach when we want to represent categorical variables with vectors that place similar categories close to one another. Other approaches for encoding categorical variables do not represent similar entities as being closer to one another, and entity embedding is a learning-based method for this important task.\n",
    "\n",
    "Overall, this project is a great look at the potential for neural networks to create meaningful embeddings of high dimensional data and a practical application of deep learning. The code itself is relatively simple, and the Keras library makes developing deep learning models enjoyable!\n",
    "\n",
    "The code here is adapted from the excellent [Deep Learning Cookbook](https://www.amazon.com/Deep-Learning-Cookbook-Practical-Recipes/dp/149199584X), the notebooks for which can be found on [this GitHub repository]((https://github.com/DOsinga/deep_learning_cookbook/tree/master)). Check out this book for practical applications of deep learning and great projects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Load in data and clean</h3>\n",
    "\n",
    "The data is stored as json with line for every book. This data contains every single book article on Wikipedia which was parsed in the [Downloading and Parsing Wikipedia Data](1-Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb) Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "# Set shell to show all lines of output\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37020 books.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "books = []\n",
    "\n",
    "with open('./data/found_books_filtered.ndjson', 'r') as file:\n",
    "    # Append each line to the books\n",
    "    books = [json.loads(row) for row in file]\n",
    "\n",
    "# Remove non-book items\n",
    "books_with_wikipedia = [book for book in books if 'Wikipedia:' in book[0]] # Articles\n",
    "books = [book for book in books if 'Wikipedia:' not in book[0]] # Books\n",
    "print(f'Found {len(books)} books.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " {'name': 'Global Wikipedia',\n",
       "  'author': 'Pnina Fichman and Noriko Hara',\n",
       "  'country': 'United States',\n",
       "  'language': 'English',\n",
       "  'subject': 'Wikipedia',\n",
       "  'publisher': 'Rowman  &  Littlefield',\n",
       "  'release_date': '2014',\n",
       "  'pages': '178',\n",
       "  'isbn': '978-0810891012'},\n",
       " ['User:Adler.fa',\n",
       "  'User:Maximilianklein',\n",
       "  'User:Piotrus',\n",
       "  'User:Kimaus',\n",
       "  'User:Tbayer (WMF)',\n",
       "  'Rowman  &  Littlefield',\n",
       "  'Indiana University Bloomington',\n",
       "  'User:Maximilianklein',\n",
       "  'User talk:Maximilianklein',\n",
       "  'File:Immanuel Kant (painted portrait).jpg',\n",
       "  'Immanuel Kant',\n",
       "  'PageRank',\n",
       "  'CheiRank',\n",
       "  'm:Research:Newsletter/2013/April#How_Wikipedia.27s_Google_matrix_differs_for_politicians_and_artists',\n",
       "  'm:Research:Newsletter/2013/July#Multilingual_ranking_analysis:_Napoleon_and_Michael_Jackson_as_Wikipedia.27s_.22global_heroes.22',\n",
       "  'DBpedia',\n",
       "  'User:Piotrus',\n",
       "  'OpenSym',\n",
       "  'Chinese Wikipedia',\n",
       "  'Baidu Baike',\n",
       "  'microblog',\n",
       "  'Twitter',\n",
       "  'Sina Weibo',\n",
       "  'Internet censorship in China',\n",
       "  'User:Kimaus',\n",
       "  'Critical mass (sociodynamics)',\n",
       "  'File:Vis gartenbaukino3.jpg',\n",
       "  'Bloomington, Indiana',\n",
       "  'Wikipedia:Wikipedia_Signpost/2012-11-26/Recent_research',\n",
       "  'File:Belgium provinces regions striped.png',\n",
       "  'Flemish Community',\n",
       "  'French Community of Belgium',\n",
       "  'German-speaking Community of Belgium',\n",
       "  'm:Research:Newsletter#How to contribute',\n",
       "  'EHEC',\n",
       "  'Category:Wikipedia Signpost archives 2014-06',\n",
       "  'Category:Wikipedia Signpost Research report archives 2014'],\n",
       " ['http://nbviewer.ipython.org/github/notconfusing/wikidataSex/blob/master/ratio_analysis/Sex%20Ratios%20By%20Language%20May%202014.ipynb',\n",
       "  'http://hci.cs.umanitoba.ca/assets/publication_files/IntelWiki_UMAP_final.pdf',\n",
       "  'http://hci.cs.umanitoba.ca/assets/publication_files/Mohammad_Noor_Nawaz_Thesis.pdf',\n",
       "  'http://hci.cs.umanitoba.ca/projects-and-research/details/intelwiki',\n",
       "  'https://www.youtube.com/watch?v=8X8aTdnYBRI',\n",
       "  'http://people.oii.ox.ac.uk/hanteng/2014/06/17/what-do-chinese-language-microblog-users-do-with-baidu-baike-and-chinese-wikipedia/',\n",
       "  'http://www.aaai.org/ocs/index.php/ICWSM/ICWSM14/paper/view/8104',\n",
       "  'http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0071226',\n",
       "  'http://journal.aall.org.au/index.php/jall/article/view/319',\n",
       "  'http://people.oii.ox.ac.uk/hanteng/2014/05/13/how-does-wikipedia-cover-the-world-differently-than-google-or-bing-2/',\n",
       "  'http://dx.doi.org/10.1080/09298215.2013.848904',\n",
       "  'http://www.skynet.ie/~arash/PDFs/JIS-2487-accepted.pdf',\n",
       "  'http://link.springer.com/article/10.1007/s11616-013-0191-z',\n",
       "  'http://ir.lib.uwo.ca/cgi/viewcontent.cgi?article=3523'],\n",
       " '2017-04-12T15:59:38Z',\n",
       " 18267]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_with_wikipedia[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Freud: His Life and His Mind',\n",
       " {'1': '< !-- See Wikipedia:WikiProject_Books -- >',\n",
       "  'name': 'Freud: His Life and His Mind',\n",
       "  'image': 'File:Freud, His Life and His Mind (first edition).jpg',\n",
       "  'caption': 'Cover of the first edition',\n",
       "  'author': 'Helen Walker Puner',\n",
       "  'country': 'United States',\n",
       "  'language': 'English',\n",
       "  'subject': 'Sigmund Freud',\n",
       "  'publisher': 'Dell Publishing',\n",
       "  'pub_date': '1947',\n",
       "  'media_type': 'Print (Hardcover and Paperback)',\n",
       "  'pages': '288 (1959 edition)',\n",
       "  'isbn': '978-1560006114'},\n",
       " ['Sigmund Freud',\n",
       "  'Dell Publishing',\n",
       "  'Hardcover',\n",
       "  'Paperback',\n",
       "  'Sigmund Freud',\n",
       "  'Erich Fromm',\n",
       "  'Dell Publishing',\n",
       "  'Anna Freud',\n",
       "  'Ernest Jones',\n",
       "  'Carl Jung',\n",
       "  'Wilhelm Stekel',\n",
       "  'Fritz Wittels',\n",
       "  'Maurice English',\n",
       "  'The Nation',\n",
       "  'Frederick Crews',\n",
       "  'The New York Review of Books',\n",
       "  'Peter Gay',\n",
       "  'Freud: A Life for Our Time',\n",
       "  'The Life and Work of Sigmund Freud',\n",
       "  'Louis Breger',\n",
       "  'Wilhelm Fliess',\n",
       "  'Freud family',\n",
       "  'Cambridge University Press',\n",
       "  'John Wiley  &  Sons',\n",
       "  'Dell Publishing',\n",
       "  'Papermac',\n",
       "  'The Nation',\n",
       "  'The New York Review of Books',\n",
       "  'Category:1947 books',\n",
       "  'Category:Books about Sigmund Freud',\n",
       "  'Category:English-language books'],\n",
       " ['https://www.ebsco.com',\n",
       "  'http://www.nybooks.com/articles/2017/02/23/freud-whats-left/'],\n",
       " '2018-04-03T19:57:42Z',\n",
       " 3069]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few articles that were caught which are clearly not books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wikipedia:Wikipedia Signpost/2014-06-25/Recent research',\n",
       " 'Wikipedia:New pages patrol/Unpatrolled articles/December 2010',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 September 23',\n",
       " 'Wikipedia:Articles for creation/Redirects/2012-10',\n",
       " 'Wikipedia:Templates for discussion/Log/2012 October 4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[book[0] for book in books_with_wikipedia][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"static\\book_template.PNG\" width=20% align ='center'>\n",
    "\n",
    "Each legitimate book contains the title, the information from the `Infobox book` (image above) template, the internal wikipedia links, the external links, the date of last edit, and the number of characters in the article (a rough estimate of the length of the article)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Title-- : Limonov (novel); \n",
      "\n",
      " -Infobox-- : {'name': 'Limonov', 'author': 'Emmanuel CarrÃ¨re', 'translator': 'John Lambert', 'country': 'France', 'language': 'French', 'publisher': 'P.O.L.', 'pub_date': '2011', 'english_pub_date': '2014', 'pages': '488', 'isbn': '978-2-8180-1405-9'};\n",
      "\n",
      " -Wikilinks-- : ['Emmanuel CarrÃ¨re', 'biographical novel', 'Emmanuel CarrÃ¨re', 'Eduard Limonov', 'Prix de la langue franÃ§aise']; \n",
      "\n",
      " -External links-- :     ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php', 'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html', 'http://limonow.de/carrere/index.html', 'http://www.tout-sur-limonov.fr/222318809'];\n",
      "\n",
      " ['http://www.lefigaro.fr/flash-actu/2011/10/05/97001-20111005FILWWW00615-le-prix-de-la-langue-francaise-a-e-carrere.php', 'http://www.lexpress.fr/culture/livre/emmanuel-carrere-prix-renaudot-2011_1046819.html', 'http://limonow.de/carrere/index.html', 'http://www.tout-sur-limonov.fr/222318809'];\n",
      "\n",
      " -Date of last edit-- : 2018-08-18T02:03:21Z;\n",
      "\n",
      " -Number of pages-- : 1437\n"
     ]
    }
   ],
   "source": [
    "n = 21 # book NÂ°21\n",
    "print(f\"-Title-- : {books[n][0]}; \\n\\n -Infobox-- : {books[n][1]};\\n\\n -Wikilinks-- : {books[n][2][:5]}; \\n\\n -External links-- : \\\n",
    "    {books[n][3][:5]};\\n\\n {books[n][3][:5]};\\n\\n -Date of last edit-- : {books[n][4]};\\n\\n -Number of pages-- : {books[n][5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Book 21\n",
    "\n",
    "<img src=\"./static/Limonov(novel).png\" width=75% align ='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General note:** We will only use the wikilinks, which are saved as the third element (index 2) for each book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Prepare data for supervised machine learning task</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Map books to integers\n",
    "\n",
    "First we want to create a mapping of book titles to integers. When we feed books into the embedding neural network, we will have to represent them as numbers, and this mapping will let us keep track of the books. We'll also create the reverse mapping, from integers back to the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22494"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Anna Karenina'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_index = {book[0]: idx for idx, book in enumerate(books)}\n",
    "index_book = {idx: book for book, idx in book_index.items()}\n",
    "\n",
    "book_index['Anna Karenina']\n",
    "index_book[22494]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Exploring Wikilinks\n",
    "\n",
    "Although it's not our main focus, we can do a little exploration. Let's find the number of unique Wikilinks and the most common ones in the entire dataset. To create a single list from a list of lists, we can use the `itertools` chain method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 311276 unique wikilinks.\n",
      "There are 17032 unique wikilinks to other books.\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "# Unique wikilinks\n",
    "wikilinks = list(chain(*[book[2] for book in books])) # list(chain(*[['a', 'b'], ['c', 'a', 'e']])) => ['a', 'b', 'c', 'a', 'e']\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "# Wikilinks that appear in more than 2 books at least\n",
    "wikilinks_other_books = [link for link in wikilinks if link in book_index.keys()]\n",
    "print(f\"There are {len(set(wikilinks_other_books))} unique wikilinks to other books.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EDA: Most Linked-to Articles\n",
    "\n",
    "Let's take a look at which pages are most linked to by books on Wikipedia.\n",
    "\n",
    "We'll make a utility function that takes in a list and returns a sorted ordered dictionary of the counts of the items in the list. The $collections$ module has a number of useful functions for dealing with groups of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def count_items(l:list):\n",
    "    \"\"\"Return ordered dictionary of counts of objects in `l`\"\"\"\n",
    "    \n",
    "    # Create a counter object\n",
    "    counts = Counter(l)\n",
    "    \n",
    "    # Sort by highest count first and place in ordered dictionary\n",
    "    counts = sorted(counts.items(), key = lambda x: x[1], reverse = True)\n",
    "    counts = OrderedDict(counts)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to count wikilinks from each book once, so we first find the set of links for each book, then we flatten the list of lists to a single list, and finally pass it to the count_items function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hardcover', 7489),\n",
       " ('Paperback', 7311),\n",
       " ('Wikipedia:WikiProject Books', 6043),\n",
       " ('Wikipedia:WikiProject Novels', 6015),\n",
       " ('English language', 4185),\n",
       " ('United States', 3060),\n",
       " ('Science fiction', 3030),\n",
       " ('The New York Times', 2727),\n",
       " ('science fiction', 2502),\n",
       " ('novel', 1979)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of wikilinks for each book and convert to a flattened list\n",
    "unique_wikilinks = list(chain(*[list(set(book[2])) for book in books]))\n",
    "\n",
    "wikilink_counts = count_items(unique_wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most linked to pages are in fact not that surprising! One thing we should notice is that there are discrepancies in capitalization. We want to normalize across capitalization, so we'll lowercase all of the links and redo the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 297624 unique wikilinks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('paperback', 8740),\n",
       " ('hardcover', 8648),\n",
       " ('wikipedia:wikiproject books', 6043),\n",
       " ('wikipedia:wikiproject novels', 6016),\n",
       " ('science fiction', 5665),\n",
       " ('english language', 4248),\n",
       " ('united states', 3063),\n",
       " ('novel', 2983),\n",
       " ('the new york times', 2742),\n",
       " ('fantasy', 2003)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilinks = [link.lower() for link in unique_wikilinks]\n",
    "print(f\"There are {len(set(wikilinks))} unique wikilinks.\")\n",
    "\n",
    "wikilink_counts = count_items(wikilinks)\n",
    "list(wikilink_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Conclusion:$ Normalizing wikilinks changes the rankings ! This illustrates an important point: always make sure to take a look at your data before modeling!ðŸ˜‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remove Most Popular Wikilinks\n",
    "\n",
    "I'm going to remove the **most popular wikilinks** because these **are not very informative**. Knowing whether a book is 'hardcover' or 'paperback' is not that important to the content. We also don't need the two Wikipedia... (wikipedia:wikiproject books, wikipedia:wikiproject novels) links since these do not distinguish the books based on content. <br/>\n",
    "\n",
    "-> Task 1: It is recommended to play around with the wikilinks that are removed because some might have a large effect on the recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(This step is similar to the idea of [TF-IDF (Term Frequency Inverse Document Frequency)](https://tfidf.com/). When dealing with words in documents, the words that appear most often across documents are usually not that helpful because they don't distinguish documents. TF-IDF is a way to weight a word higher for appearing more often within an article but decrease the weighting for a word appearing more often between articles.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_remove = ['hardcover', 'paperback', 'hardback', 'e-book', 'wikipedia:wikiproject books', 'wikipedia:wikiproject novels']\n",
    "for t in to_remove:\n",
    "    wikilinks.remove(t)\n",
    "    _ = wikilink_counts.pop(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are so many unique wikilinks, I'm going to limit the list to wikilinks mentioned 4 or more times. Hopefully this reduces the noise that might come from wikilinks that only appear a few times. Keeping every single link will increase the training time significantly, but experiment with this parameter if you are interested.\n",
    "\n",
    "-> Task 2: Try different treshold for wikilinks appearance then analyse results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41758\n"
     ]
    }
   ],
   "source": [
    "# Limit to greater or equal than 4 links\n",
    "links = [t[0] for t in wikilink_counts.items() if t[1] >= 4]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Conclusion$: By removing meaningless wikilinks (wikilinks that appear almost every time and those that do not link to minimum 4 books), we drastically reduce noises in our data. We have now 41758 unique wikilinks instead of 297624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> EDA: Most Linked-to Books\n",
    "\n",
    "As a final bit of exploration, let's look at the books that are mentioned the most by other books on Wikipedia. We'll take the set of links for each book so that we don't have multiple counts for books that are linked to by another book more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The Encyclopedia of Science Fiction', 127),\n",
       " ('The Discontinuity Guide', 104),\n",
       " ('The Encyclopedia of Fantasy', 63),\n",
       " ('Dracula', 55),\n",
       " ('EncyclopÃ¦dia Britannica', 51),\n",
       " ('Nineteen Eighty-Four', 51),\n",
       " ('Don Quixote', 49),\n",
       " ('The Wonderful Wizard of Oz', 49),\n",
       " (\"Alice's Adventures in Wonderland\", 47),\n",
       " ('Jane Eyre', 39)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find set of book wikilinks for each book\n",
    "unique_wikilinks_books = list(chain(*[list(set(link for link in book[2] if link in book_index.keys())) for book in books]))\n",
    "\n",
    "# Count the number of books linked to by other books\n",
    "wikilink_book_counts = count_items(unique_wikilinks_books)\n",
    "list(wikilink_book_counts.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not surprising that several of these are references (\"Encyclopedia....\"). We also see that a few classics make it into the list (\"The Wonderful Wizard of Oz\", \"Alice's Adventures in Wonderland\")!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Potential Additional Cleaning Step\n",
    "\n",
    "Annother option to try more data cleaning would be to clean the link entities. For example, both `the new york times` and `new york times` are in the links. These could clearly be combined into a single entry because they link to the same exact page. This might require manual inspection of the links, and I decided not to do this because of the time involved! The final embeddings turned out well even without this step, but it might make sense to do in the future.\n",
    "\n",
    "I'm not sure why the same link is represented as two different names (I extracted the title of the link to try and alleviate this issue), but it occurs many times, even for the same book!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Title-- : The Big Picture: Who Killed Hollywood? and Other Essays\n",
      "-Wikilinks-- : ['Wikipedia:WikiProject Novels', 'Wikipedia:WikiProject Books', 'William Goldman', 'United States', 'English language', 'William Goldman', 'Michael Sragow', 'Good Will Hunting', 'Robin Williams', 'Matt Damon', 'The New York Times', 'The New York Times Company', 'New York Times', 'Category:Cinema of the United States', 'Category:Film production', 'Category:2000 books', 'Category:Books about films', 'Category:Books by William Goldman', 'Category:Show business memoirs']\n"
     ]
    }
   ],
   "source": [
    "for book in books:\n",
    "    if 'The New York Times' in book[2] and 'New York Times' in book[2]:\n",
    "        print(f\"-Title-- : {book[0]}\\n-Wikilinks-- : {book[2]}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2742"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikilink_counts.get('the new york times')\n",
    "wikilink_counts.get('new york times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> Task3: Try some additionnal cleanning techniques (by inspecting some sample for example) in order to combine words as `The new york times` and `new york times` into single entry as those words refer to the same wikilink."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Wikilinks to index\n",
    "\n",
    "As with the books, we need to map the Wikilinks to integers. We'll also create the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the economist'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 41758 wikilinks that will be used.\n"
     ]
    }
   ],
   "source": [
    "link_index = {link: idx for idx, link in enumerate(links)}\n",
    "index_link = {idx: link for link, idx in link_index.items()}\n",
    "\n",
    "link_index['the economist']\n",
    "index_link[300]\n",
    "print(f'There are {len(link_index)} wikilinks that will be used.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Supervised Machine Learning Task</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean data, we'll move on to the second step: developing a supervised machine learning task to train an embedding neural network. As a reminder, we'll state the problem as: given a book title and a link, identify if the link is in the book's article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Build a training set\n",
    "\n",
    "In order for any machine learning model to learn, it needs a training set. We are going to treat this as a supervised learning problem: given a pair (book, link), we want the neural network to learn to predict whether this is a legitimate pair - present in the data - or not.\n",
    "\n",
    "To create a training set, for each book, we'll iterate through the wikilinks on the book page and record the book title and each link as a tuple. The final pairs list will consist of tuples of every (book, link) pairing on all of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772798, 41758, 37020)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(321, 232)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# pairs = []\n",
    "\n",
    "# # Iterate through each book\n",
    "# for book in books:\n",
    "#     # Iterate through the links in the book\n",
    "#     pairs.extend((book_index[book[0]], link_index[link.lower()]) \\\n",
    "#                 for link in book[2] if link.lower() in links)\n",
    "\n",
    "# # Serialize\n",
    "# fileObj = open('pairs.obj', 'wb')\n",
    "# pickle.dump(pairs,fileObj)\n",
    "# fileObj.close()\n",
    "\n",
    "# Deserialize\n",
    "fileObj = open('pairs.obj', 'rb')\n",
    "pairs = pickle.load(fileObj)\n",
    "fileObj.close()\n",
    "\n",
    "len(pairs), len(links), len(books)\n",
    "pairs[5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have over 770,000 <font color='green'>positive examples</font> on which to train! Each pair represents one Wikilink for one book. Let's look at a few examples.\n",
    "\n",
    "Later on we'll create the <font color='red'>negative examples</font> by randomly sampling from the links and the books and making sure the resulting pair is not in pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Slaves in the Family', 'category:american biographies')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Man Who Watched the Trains Go By (novel)',\n",
       " 'category:belgian novels adapted into films')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[pairs[5000][0]], index_link[pairs[5000][1]]\n",
    "index_book[pairs[900][0]], index_link[pairs[900][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_set = set(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the (book, link) pairs that are represented most often in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((13337, 31111), 85),\n",
       " ((31899, 65), 77),\n",
       " ((25899, 8850), 61),\n",
       " ((1851, 2629), 57),\n",
       " ((25899, 30465), 53)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Counter(pairs)\n",
    "sorted(x.items(), key=lambda x: x[1], reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"France's Songs of the Bards of the Tyne - 1850\", 'hydrophobie (song)')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('The Early Stories: 1953â€“1975', 'the new yorker')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "('Marthandavarma (novel)', 'kerala sahitya akademi')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_book[13337], index_link[31111]\n",
    "index_book[31899], index_link[65]\n",
    "index_book[25899], index_link[30465]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's nothing wrong with books that link to the same page many times. They are just more likely to be trained on since there are more of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important note** about Training/Testing Set !!!\n",
    "\n",
    "As we said before, to compute the embeddings, we are not going to create a separate validation or testing set. While this is a **must** for a normal supervised machine learning task, in this case, our primary objective is not to make the most accurate model, but to generate the best embeddings. The prediction task is just the method through which we train our network to make the embeddings. At the end of training, we are not going to be testing our model on new data, so we don't need to evaluate the performance. Instead of testing on new data, we'll look at the embeddings themselves to see if books that we think are similar have embeddings that are close to each other.\n",
    "\n",
    "If we kept a separate validation / testing set, then we would be limiting the amount of data that our network can use to train. This would result in less accurate embeddings. Normally with any supervised model, we need to be concerned about overfitting, but again, because we do not need our model to generalize to new data and our goal is the embeddings, we will make our model as effective as possible by using all the data for training. In general, always have a separate validation and testing set (or use cross validation) and make sure to regularize your model to prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generator for training samples\n",
    "\n",
    "We need to generate positive samples and negative samples to train the neural network. The positive samples are simple: pick a pair from `pairs` and assign it a 1. The negative samples are also fairly easy: pick one random link and one random book, make sure they are not in `pairs`, and assign them a -1 or a 0. (We'll use either a -1 or 0 for the negative labels depending on whether we want to make this a regression or a classification problem. Either approach is valid, and we'll try out both methods.)\n",
    "\n",
    "The code below creates a generator that yields batches of samples each time it is called. Neural networks are trained incrementally - a batch at a time - which means that a generator is a useful function for returning examples on which to train. Using a generator alleviates the need to store all of the training data in memory which might be an issue if we were working with a larger dataset such as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(100)\n",
    "\n",
    "def generate_batch(pairs, n_positive = 50, negative_ratio = 1.0, classification = False):\n",
    "    \"\"\"Generate batches of samples for training\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    * pairs:\n",
    "    * n_positive:\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A generator of batches. Should call `next` on this object to get a new batch\n",
    "    \"\"\"\n",
    "    batch_size = n_positive * (1 + negative_ratio)\n",
    "    batch = np.zeros((batch_size, 3))\n",
    "    \n",
    "    # Adjust label based on task\n",
    "    if classification:\n",
    "        neg_label = 0\n",
    "    else:\n",
    "        neg_label = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Build the entity embedding neural network</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train the neural network on prediction task\n",
    "5. Extract embeddings and find most similar books and wikilinks\n",
    "6. Visualize the embeddings using dimension reduction techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wikipedia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
